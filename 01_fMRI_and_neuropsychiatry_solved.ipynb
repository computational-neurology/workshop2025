{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1cp5OCnPd7E"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/computational-neurology/workshop2025/blob/main/01_fMRI_and_neuropsychiatry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqqrWtRcrriv"
      },
      "source": [
        "# Introduction to fMRI, neuroanatomy and neuropsychiatry\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dy1AOXjrri6"
      },
      "source": [
        "### Let's compute functional connectivity!\n",
        "\n",
        "Nilearn is a Python toolbox that has a built in function for extracting timeseries from functional files and doing all the extra signal processing at the same time. Let's walk through how this is done.\n",
        "\n",
        "First we'll grab our imports to have all function that we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-YRuxucvrri7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nilearn in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (0.10.1)\n",
            "Requirement already satisfied: packaging in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (23.1)\n",
            "Requirement already satisfied: requests>=2.25.0 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (2.30.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (1.24.3)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (1.2.0)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: nibabel>=3.2.0 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (5.1.0)\n",
            "Requirement already satisfied: lxml in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from nilearn) (4.9.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from pandas>=1.1.5->nilearn) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from requests>=2.25.0->nilearn) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from requests>=2.25.0->nilearn) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from requests>=2.25.0->nilearn) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from requests>=2.25.0->nilearn) (3.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from scikit-learn>=1.0.0->nilearn) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/riccardo/anaconda3/envs/neurolib/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->nilearn) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "#! pip install numpy\n",
        "#! pip install matplotlib\n",
        "#! pip install pandas\n",
        "!pip install nilearn\n",
        "#! pip install bids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s4wAu6k4rri9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nilearn import image\n",
        "from nilearn import datasets\n",
        "from nilearn import plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Xn8Fyerri9"
      },
      "source": [
        "Let's grab the data that we want to perform our connectivity analysis later on. Letâ€™s define the regions we want to extract the signal from. We will use the Harvard-Oxford atlas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "36raBbWgrri-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset created in /home/riccardo/nilearn_data/fsl\n",
            "\n",
            "Downloading data from https://www.nitrc.org/frs/download.php/9902/HarvardOxford.tgz ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 1384448 of 25716861 bytes (5.4%,  3.3min remaining)"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/riccardo/workshop2025/01_fMRI_and_neuropsychiatry_solved.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/riccardo/workshop2025/01_fMRI_and_neuropsychiatry_solved.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mfetch_atlas_harvard_oxford(\u001b[39m'\u001b[39;49m\u001b[39mcort-maxprob-thr25-2mm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/riccardo/workshop2025/01_fMRI_and_neuropsychiatry_solved.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m atlas_filename \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmaps\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-18.04/home/riccardo/workshop2025/01_fMRI_and_neuropsychiatry_solved.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m labels \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mlabels\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/nilearn/datasets/atlas.py:497\u001b[0m, in \u001b[0;36mfetch_atlas_harvard_oxford\u001b[0;34m(atlas_name, data_dir, symmetric_split, resume, verbose)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m is_probabilistic \u001b[39mand\u001b[39;00m symmetric_split:\n\u001b[1;32m    489\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRegion splitting not supported for probabilistic atlases\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m     )\n\u001b[1;32m    492\u001b[0m (\n\u001b[1;32m    493\u001b[0m     atlas_img,\n\u001b[1;32m    494\u001b[0m     atlas_filename,\n\u001b[1;32m    495\u001b[0m     names,\n\u001b[1;32m    496\u001b[0m     is_lateralized,\n\u001b[0;32m--> 497\u001b[0m ) \u001b[39m=\u001b[39m _get_atlas_data_and_labels(\n\u001b[1;32m    498\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mHarvardOxford\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    499\u001b[0m     atlas_name,\n\u001b[1;32m    500\u001b[0m     symmetric_split\u001b[39m=\u001b[39;49msymmetric_split,\n\u001b[1;32m    501\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m    502\u001b[0m     resume\u001b[39m=\u001b[39;49mresume,\n\u001b[1;32m    503\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    504\u001b[0m )\n\u001b[1;32m    505\u001b[0m atlas_niimg \u001b[39m=\u001b[39m check_niimg(atlas_img)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m symmetric_split \u001b[39mor\u001b[39;00m is_lateralized:\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/nilearn/datasets/atlas.py:699\u001b[0m, in \u001b[0;36m_get_atlas_data_and_labels\u001b[0;34m(atlas_source, atlas_name, symmetric_split, data_dir, resume, verbose)\u001b[0m\n\u001b[1;32m    695\u001b[0m label_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(root, label_file)\n\u001b[1;32m    696\u001b[0m atlas_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    697\u001b[0m     root, atlas_source, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00matlas_source\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00matlas_name\u001b[39m}\u001b[39;00m\u001b[39m.nii.gz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    698\u001b[0m )\n\u001b[0;32m--> 699\u001b[0m atlas_file, label_file \u001b[39m=\u001b[39m _fetch_files(\n\u001b[1;32m    700\u001b[0m     data_dir,\n\u001b[1;32m    701\u001b[0m     [(atlas_file, url, opts), (label_file, url, opts)],\n\u001b[1;32m    702\u001b[0m     resume\u001b[39m=\u001b[39;49mresume,\n\u001b[1;32m    703\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    704\u001b[0m )\n\u001b[1;32m    705\u001b[0m \u001b[39m# Reorder image to have positive affine diagonal\u001b[39;00m\n\u001b[1;32m    706\u001b[0m atlas_img \u001b[39m=\u001b[39m reorder_img(atlas_file)\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/nilearn/datasets/utils.py:786\u001b[0m, in \u001b[0;36m_fetch_files\u001b[0;34m(data_dir, files, resume, verbose, session)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[39mwith\u001b[39;00m requests\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m    785\u001b[0m         session\u001b[39m.\u001b[39mmount(\u001b[39m\"\u001b[39m\u001b[39mftp:\u001b[39m\u001b[39m\"\u001b[39m, _NaiveFTPAdapter())\n\u001b[0;32m--> 786\u001b[0m         \u001b[39mreturn\u001b[39;00m _fetch_files(\n\u001b[1;32m    787\u001b[0m             data_dir,\n\u001b[1;32m    788\u001b[0m             files,\n\u001b[1;32m    789\u001b[0m             resume\u001b[39m=\u001b[39;49mresume,\n\u001b[1;32m    790\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    791\u001b[0m             session\u001b[39m=\u001b[39;49msession,\n\u001b[1;32m    792\u001b[0m         )\n\u001b[1;32m    793\u001b[0m \u001b[39m# There are two working directories here:\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \u001b[39m# - data_dir is the destination directory of the dataset\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[39m# - temp_dir is a temporary directory dedicated to this fetching call. All\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[39m#   files that must be downloaded will be in this directory. If a corrupted\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[39m#   file is found, or a file is missing, this working directory will be\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[39m#   deleted.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m files \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(files)\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/nilearn/datasets/utils.py:845\u001b[0m, in \u001b[0;36m_fetch_files\u001b[0;34m(data_dir, files, resume, verbose, session)\u001b[0m\n\u001b[1;32m    842\u001b[0m     os\u001b[39m.\u001b[39mmkdir(temp_dir)\n\u001b[1;32m    843\u001b[0m md5sum \u001b[39m=\u001b[39m opts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmd5sum\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 845\u001b[0m dl_file \u001b[39m=\u001b[39m _fetch_file(\n\u001b[1;32m    846\u001b[0m     url,\n\u001b[1;32m    847\u001b[0m     temp_dir,\n\u001b[1;32m    848\u001b[0m     resume\u001b[39m=\u001b[39;49mresume,\n\u001b[1;32m    849\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    850\u001b[0m     md5sum\u001b[39m=\u001b[39;49mmd5sum,\n\u001b[1;32m    851\u001b[0m     username\u001b[39m=\u001b[39;49mopts\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39musername\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    852\u001b[0m     password\u001b[39m=\u001b[39;49mopts\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mpassword\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    853\u001b[0m     session\u001b[39m=\u001b[39;49msession,\n\u001b[1;32m    854\u001b[0m     overwrite\u001b[39m=\u001b[39;49moverwrite,\n\u001b[1;32m    855\u001b[0m )\n\u001b[1;32m    856\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmove\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m opts:\n\u001b[1;32m    857\u001b[0m     \u001b[39m# XXX: here, move is supposed to be a dir, it can be a name\u001b[39;00m\n\u001b[1;32m    858\u001b[0m     move \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(temp_dir, opts[\u001b[39m\"\u001b[39m\u001b[39mmove\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/nilearn/datasets/utils.py:667\u001b[0m, in \u001b[0;36m_fetch_file\u001b[0;34m(url, data_dir, resume, overwrite, md5sum, username, password, verbose, session)\u001b[0m\n\u001b[1;32m    665\u001b[0m         resp\u001b[39m.\u001b[39mraise_for_status()\n\u001b[1;32m    666\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(temp_full_name, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fh:\n\u001b[0;32m--> 667\u001b[0m             _chunk_read_(\n\u001b[1;32m    668\u001b[0m                 resp,\n\u001b[1;32m    669\u001b[0m                 fh,\n\u001b[1;32m    670\u001b[0m                 report_hook\u001b[39m=\u001b[39;49m(verbose \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m),\n\u001b[1;32m    671\u001b[0m                 initial_size\u001b[39m=\u001b[39;49minitial_size,\n\u001b[1;32m    672\u001b[0m                 verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m shutil\u001b[39m.\u001b[39mmove(temp_full_name, full_name)\n\u001b[1;32m    675\u001b[0m dt \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/nilearn/datasets/utils.py:172\u001b[0m, in \u001b[0;36m_chunk_read_\u001b[0;34m(response, local_file, chunk_size, report_hook, initial_size, total_size, verbose)\u001b[0m\n\u001b[1;32m    169\u001b[0m bytes_so_far \u001b[39m=\u001b[39m initial_size\n\u001b[1;32m    171\u001b[0m t0 \u001b[39m=\u001b[39m time_last_display \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 172\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39miter_content(chunk_size):\n\u001b[1;32m    173\u001b[0m     bytes_so_far \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(chunk)\n\u001b[1;32m    174\u001b[0m     time_last_read \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 935\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    937\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    938\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/urllib3/response.py:874\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    872\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 874\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    876\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/urllib3/response.py:809\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    806\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 809\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    811\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    818\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    819\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/site-packages/urllib3/response.py:794\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    792\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/anaconda3/envs/neurolib/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
        "atlas_filename = dataset.maps\n",
        "labels = dataset.labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhucND27rri_"
      },
      "source": [
        "Let's have a look at the brain parcellation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xCP9PJRrri_"
      },
      "outputs": [],
      "source": [
        "plotting.plot_roi(atlas_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaJmcllWrri_"
      },
      "source": [
        "Let's now look at the names of the first ten brain regions of the parcellation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnr3cFQNrrjA"
      },
      "outputs": [],
      "source": [
        "# Load labels for each atlas region\n",
        "labels[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGPTvmJFrrjA"
      },
      "source": [
        "We can also look at the fMRI data from the dataset. Let's have a look at the different files associated with the fMRI data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-EkpONOrrjA"
      },
      "outputs": [],
      "source": [
        "# One subject of fmri data\n",
        "data = datasets.fetch_development_fmri(n_subjects=1)\n",
        "func_file = data.func[0]\n",
        "confound_file = data.confounds[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HT2301vrrjB"
      },
      "source": [
        "We can now have a look at the mean image of the fMRI file. You can see that it is quite blurry- the resolution is much lower than of an anatomical file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0l34sqSrrjB"
      },
      "outputs": [],
      "source": [
        "plotting.view_img(image.mean_img(func_file), threshold=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogIOHu2UrrjC"
      },
      "source": [
        "Now we'll import a package from <code>nilearn</code>, called <code>input_data</code> which allows us to extract the time series data using the parcellation file (i.e., the average signal over time from each of the regions of the parcellations will be extracted), and at the same time applying data preprocessing to clean the time series!\n",
        "\n",
        "We first create an object using the parcellation file <code>Harvard-Oxford atlas</code> and our cleaning settings which are the following:\n",
        "\n",
        "Settings to use:\n",
        "- Confounds: trans_x, trans_y, trans_z, rot_x, rot_y, rot_z, white_matter, csf, global_signal\n",
        "- Temporal Derivatives: Yes\n",
        "- high_pass = 0.009\n",
        "- low_pass = 0.08\n",
        "- detrend = True\n",
        "- standardize = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HtQ5gPfrrjC"
      },
      "source": [
        "To extract signal on the parcellation you can use the NiftiLabelsMasker in Python. It is a processing object that is created by specifying all the important parameters (e.g, how to preprocess the data), but not the data itself. You have to give the data to the masker later on and you can use it on as many functional imaging data as you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRcwKLK5rrjC"
      },
      "outputs": [],
      "source": [
        "# Note: if you are using Nilearn with a version newer than 0.9.0 (e.g., in the future on your local laptop),\n",
        "#then you should use the following: from nilearn.maskers import NiftiLabelsMasker\n",
        "\n",
        "from nilearn.input_data import NiftiLabelsMasker\n",
        "\n",
        "masker = NiftiLabelsMasker(labels_img=atlas_filename,\n",
        "                                      standardize=True,\n",
        "                                      memory='nilearn_cache',\n",
        "                                      verbose=1,\n",
        "                                      detrend=True,\n",
        "                                     low_pass = 0.08,\n",
        "                                     high_pass = 0.009,\n",
        "                                      t_r=2\n",
        "                                    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E9mRzKJrrjC"
      },
      "source": [
        "The object <code>masker</code> is now able to be used on *any functional image of the same size*. The `input_data.NiftiLabelsMasker` object is a wrapper that applies parcellation, cleaning and averaging to an functional image. For example let's apply this to our first subject:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnT8AibTrrjD"
      },
      "source": [
        "### Using the masker\n",
        "Finally with everything set up, we can now use the masker to perform our:\n",
        "1. Confounds cleaning\n",
        "2. Parcellation\n",
        "3. Extract the average time series from one parcel (i.e., one brain region)<p>\n",
        "All in one step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr4AhzuSrrjD"
      },
      "outputs": [],
      "source": [
        "#Apply cleaning, parcellation and extraction to functional data\n",
        "time_series = masker.fit_transform(func_file, confounds=confound_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hV9hGm_rrjD"
      },
      "source": [
        "Now we can have a look at the time series of one of the regions from the functional data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HuLMRy1rrjD"
      },
      "outputs": [],
      "source": [
        "plt.plot(time_series[:,1])\n",
        "# if you want to look at another region, just change the number 1 to another number in the code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbmkWgvDrrjE"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Exercise 1</b><p>\n",
        "Now let's look at the time series structure. What do these two values (168,48) mean?\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIV7fCtmrrjE"
      },
      "outputs": [],
      "source": [
        "time_series.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# timepoints, regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwHCFTtWrrjE"
      },
      "source": [
        "The result of running <code>masker.fit_transform</code> is a matrix that has: <p>\n",
        "INSERT ANSWER HERE\n",
        "- Rows matching the XXXX (168)\n",
        "- Columns matching the XXXX (48)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ROIorlrrrjE"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "-End of Exercise-\n",
        "            </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzMjRqrCrrjE"
      },
      "source": [
        "We originally had **49 ROIs**, what happened to 1 of them? It turns out that <code>masker</code> drops ROIs that are empty (i.e contain no brain voxels inside of them), this means that 1 of our atlas' parcels did not correspond to any region with signal! To see which ROIs are kept after computing a parcellation you can look at the <code>labels_</code> property of <code>masker</code>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2vyv0PyrrjF"
      },
      "outputs": [],
      "source": [
        "print(masker.labels_)\n",
        "print(\"Number of labels\", len(masker.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1fTdeGWrrjF"
      },
      "source": [
        "If you want to work with multiple subjects (all of whom might have different missing values), you should create an array that contains the all regions (and has zeros for the missing values). This is not part of our class today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDaaQrIJrrjJ"
      },
      "source": [
        "### Calculating Connectivity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFgwJEwyrrjK"
      },
      "source": [
        "We now have 48 time series from 48 regions, so we can see which regions have similar activity over time, i.e., are functionally connected to each other. This does not mean that they are directly connected via fibers, but they can be also connected via another region."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsYnmaTBrrjK"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Exercise 2</b><p>\n",
        "\n",
        "There are different types of connectivity that we will discuss:\n",
        "    1. ROI-based connectivity\n",
        "    2. Independent component analysis.\n",
        "Please have a look inside this paper: http://www.ajnr.org/content/early/2018/01/18/ajnr.A5527 and explain the difference between these two.\n",
        "    \n",
        "    \n",
        "-End of Exercise-  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg_arxJhrrjK"
      },
      "source": [
        "In our seminar today, we will focus on the ROI-based connectivity, i.e. we will see how each region (of interest = ROI) from a parcellation is functionally connected with each other. We will calculate a *full connectivity matrix* by computing the correlation between *all pairs of ROIs* in our parcellation scheme!\n",
        "\n",
        "We'll use another nilearn tool called <code>ConnectivityMeasure</code> from <code>nilearn.connectome</code>. This tool will perform the full set of pairwise correlations for us"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf3im5S0rrjK"
      },
      "source": [
        "Like the masker, we need to make an object that will calculate connectivity for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx427EohrrjK"
      },
      "outputs": [],
      "source": [
        "from nilearn.connectome import ConnectivityMeasure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_qxXhlXrrjL"
      },
      "outputs": [],
      "source": [
        "correlation_measure = ConnectivityMeasure(kind='correlation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSUjqCPtrrjL"
      },
      "source": [
        "Then we use <code>correlation_measure.fit_transform()</code> in order to calculate the full correlation matrix for our parcellated data!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp6U1TIPrrjL"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Exercise 3</b><p>\n",
        "\n",
        "Please now include the missing variable (i.e., the preprocessed time series from above) in the code below to calculate the connectivity.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tl6mBG3rrjL"
      },
      "outputs": [],
      "source": [
        "full_correlation_matrix = correlation_measure.fit_transform(time_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgAMNSZ3rrjL"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "-End of Exercise-  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvZ65jCqKEly"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Exercise 3a (optional)</b><p>\n",
        "\n",
        "If you have time, how can you calcuate functional connectivity without a Nilearn function (i.e., just the correlations of the time series)? E.g., which Numpy or scikit-learn functions would be suitable to retrieve pairwise correlations or even ICA?\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqCP52EZW7Kh"
      },
      "outputs": [],
      "source": [
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gylCsQk1KC8T"
      },
      "outputs": [],
      "source": [
        "fc = np.corrcoef(time_series)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjxNILn2KS6x"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "-End of Exercise-  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "565zd2TirrjL"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Exercise 4</b><p>\n",
        "Let's now look at the shape of the resulting connectivity matrix. What does it mean?\n",
        "    \n",
        "INSERT ANSWER HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmTiC8hnrrjL"
      },
      "outputs": [],
      "source": [
        "full_correlation_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLK7KaprrjM"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "-End of Exercise-  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIXZG06JrrjM"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Exercise 5</b><p>\n",
        "\n",
        "Below you find a visualization of the connectivity matrix. How would you describe it?\n",
        "    \n",
        "INSERT ANSWER HERE\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0xTjBoIJX3Q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtoSi_QsrrjM"
      },
      "outputs": [],
      "source": [
        "plt.imshow(np.squeeze(full_correlation_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfdLGocCrrjM"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "-End of Exercise-  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kOiGIworrjN"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<b>Exercise 6</b><p>\n",
        "\n",
        "Now that you know functional connectivity. It gives a value for each pair of brain regions over time. Can you split your team into two and one half does a search on  dynamic functional connectivity and the other on effective connectivity and then explains to the others? What are its advantages and disadvantages?\n",
        "    \n",
        "INSERT YOUR THOUGHTS HERE\n",
        "\n",
        "-End of Exercise-  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XohxMiRqrrjN"
      },
      "source": [
        "## Congratulations!\n",
        "\n",
        "Hopefully now you understand that:\n",
        "\n",
        "1. fMRI data needs to be pre-processed before analyzing\n",
        "2. Manipulating images in python is easily done using `nilearn` and `nibabel`\n",
        "3. You can also do post-processing like confound/nuisance regression using `nilearn`\n",
        "4. Parcellating is a method of simplifying and \"averaging\" data. The type of parcellation reflect assumptions you make about the structure of your data\n",
        "5. Functional Connectivity is really just time-series correlations between two signals!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmQ7jy75rrjN"
      },
      "source": [
        "## More information/additional reading material:\n",
        "\n",
        "https://andysbrainbook.readthedocs.io/en/latest/\n",
        "\n",
        "https://nilearn.github.io/user_guide.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
